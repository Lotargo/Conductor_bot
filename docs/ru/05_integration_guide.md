# Руководство по интеграции

Это руководство представляет практический, концептуальный пример того, как интегрировать Sentio Engine в типичное приложение чат-бота на основе LLM.

## Основная концепция: «Эмоциональный пульс»

Sentio Engine не предназначен для однократного вызова функции. Он спроектирован как постоянный сервис, обеспечивающий «эмоциональный пульс» для вашего ИИ. Общий рабочий процесс представляет собой непрерывный цикл:

1.  **Анализ** пользовательского ввода для создания `Stimulus`.
2.  **Отправка** `Stimulus` в Sentio Engine.
3.  **Получение** обновленного `Report` от Sentio Engine.
4.  **Внедрение** `Report` в контекст LLM.
5.  **Генерация** эмоционально осознанного ответа.

## Пример потока работы приложения

Давайте представим бэкенд чат-бота, созданный на Python.

### 1. Функция LLM-парсера

Сначала вам понадобится функция, которая сможет перевести текст пользователя в эмоциональный стимул. Это идеальная задача для LLM. Вы можете использовать локальную модель или API, например, OpenAI/Anthropic, со специальным промптом.

```python
def text_to_stimulus(user_text: str) -> Stimulus:
    """
    Использует LLM для анализа текста и определения эмоционального содержания.
    (Это упрощенный пример).
    """
    # Этот промпт просит LLM действовать как анализатор настроений
    # и вернуть JSON-объект.
    prompt = f"""
        Проанализируйте эмоциональное содержание следующего текста.
        Ответьте ТОЛЬКО JSON-объектом, сопоставляющим названия эмоций с интенсивностью
        от 0.0 до 1.0.

        Текст: "{user_text}"

        Пример ответа: {{"радость": 0.8, "благодарность": 0.6}}
    """

    # --- Вызов LLM API ---
    # response_json = call_llm_api(prompt)
    # mock_response = {"радость": 0.7, "доверие": 0.4} # Заглушка ответа

    stimulus = Stimulus()
    # stimulus.emotions.update(mock_response)

    return stimulus
```

### 2. Основной обработчик чата

Ваш основной обработчик чата будет организовывать весь процесс.

```python
# Предположим, что sentio_client — это вспомогательный класс, который обрабатывает запросы к API Sentio Engine
# (см. Справочник по API для примера)

def handle_chat_message(user_message: str):
    """
    Основная функция для обработки сообщения пользователя и генерации ответа.
    """
    # 1. Анализ пользовательского ввода
    stimulus = text_to_stimulus(user_message)

    # 2. Отправка стимула в Sentio Engine
    sentio_client.send_stimulus(stimulus)

    # 3. Получение обновленного эмоционального отчета
    report = sentio_client.get_report()

    # 4. Внедрение отчета в системный промпт LLM
    system_prompt = f"""
        Вы — полезный и эмпатичный AI-ассистент.
        Ваше текущее эмоциональное состояние следующее:
        - Основное настроение: {report.emotional_state.primary_mood}
        - Активные эмоции: {dict(report.emotional_state.emotions)}

        Пожалуйста, адаптируйте свой ответ, чтобы он отражал это эмоциональное состояние.
        Например, если вы чувствуете радость, будьте более восторженным.
        Если вы чувствуете грусть, будьте более сдержанным и мягким.
    """

    # 5. Генерация финального ответа
    # final_response = call_main_llm(system_prompt, user_message)

    # return final_response
```

### Почему это работает

Внедряя эмоциональное состояние непосредственно в системный промпт, вы даете LLM критически важный контекст о том, *как* ему следует себя вести. Это гораздо эффективнее, чем просто говорить ему «быть эмпатичным». Вы предоставляете ему конкретное внутреннее состояние, которое естественным образом окрасит его язык, тон и даже его рассуждения.

Этот простой цикл превращает ваш ИИ из stateless-обработчика информации в динамическую сущность с постоянной, развивающейся личностью.
